{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Spark™ is a unified analytics engine for large-scale data processing.\n",
    "![](https://spark.apache.org/images/spark-logo-trademark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://cc-media-foxit.fichub.com/image/fox-it-mondofox/0177f439-3c0f-44ae-9803-c25f8bfac0dd/flash-vs-superman-game-2jpg-maxw-824.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Run workloads 100x faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Logistic Regression](https://spark.apache.org/images/logistic-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Apache Spark achieves "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- high performance for both batch and streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- using a state-of-the-art DAG scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- a query optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- a physical execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Spark is faster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In - Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://external-preview.redd.it/RVpCIxhliY2p5vKF8I-AoCLIoI48yIEpVPXDduTG6Fc.jpg?auto=webp&s=88a001359893e5533423e9886d4d55cfd2dbdf62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://miro.medium.com/max/4096/1*KiC1gf3x3Ia_2PBYqfkLBg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ease of Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](http://www.quickmeme.com/img/4d/4d4759d82ce65de86834ff151bc8b419f89f4e2f0d003f10a54b236785e3e6d2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Write applications quickly in Java, Scala, Python, R, and SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark offers over 80 high-level operators that make it easy to build parallel apps. \n",
    "\n",
    "And you can use it **interactively** from the Scala, Python, R, and SQL shells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Scala Example\n",
    "\n",
    "\n",
    "```scala\n",
    "df = spark.read.json(\"logs.json\") \n",
    "df.where(\"age > 21\").select(\"name.first\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/3/3d/%D0%92%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%B0_%D0%B3%D0%BE%D0%B4%D0%B0._%D0%94%D0%B6%D1%83%D0%B7%D0%B5%D0%BF%D0%BF%D0%B5_%D0%90%D1%80%D1%87%D0%B8%D0%BC%D0%B1%D0%BE%D0%BB%D1%8C%D0%B4%D0%BE.jpg)\n",
    "\n",
    "> Arcimboldo - 1563 - Quattro stagioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combine SQL, streaming, and complex analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark powers a stack of libraries including "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- SQL and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- MLlib for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spark Streaming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can combine these libraries seamlessly in the same application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://spark.apache.org/images/spark-stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Runs everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://www.nextme.it/images/societa/next-economy/Criceti_umani.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes\n",
    "![](https://spark.apache.org/images/spark-runs-everywhere.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### It can access diverse external data sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Analyse\n",
    "Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Query\n",
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```Dockerfile\n",
    "FROM openjdk:8-jre\n",
    "ENV PATH $SPARK_DIR/bin:$PATH\n",
    "ENV SPARK_VERSION=3.2.1\n",
    "ENV SPARK_DIR=/opt/spark\n",
    "ENV PATH $SPARK_DIR/bin:$PATH\n",
    "\n",
    "ADD setup/spark-${SPARK_VERSION}-bin-hadoop3.2.tgz /opt\n",
    "\n",
    "RUN apt-get update && apt-get -y install bash python3 python3-pip netcat\n",
    "\n",
    "RUN pip3 install pyspark numpy elasticsearch\n",
    "# Create Sym Link \n",
    "RUN ln -s /opt/spark-${SPARK_VERSION}-bin-hadoop3.2 ${SPARK_DIR} \n",
    "\n",
    "ADD dataset /opt/tap/spark/dataset\n",
    "# Add Python Code\n",
    "ADD code/*  /opt/tap/\n",
    "# Add Java Code\n",
    "ADD apps /opt/tap/apps\n",
    "# Add Spark Manager\n",
    "ADD spark-manager.sh $SPARK_DIR/bin/spark-manager\n",
    "\n",
    "WORKDIR ${SPARK_DIR}\n",
    "ENTRYPOINT [ \"spark-manager\" ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "[[ -z \"${SPARK_ACTION}\" ]] && { echo \"SPARK_ACTION required\"; exit 1; }\n",
    "\n",
    "# ACTIONS start-zk, start-kafka, create-topic, \n",
    "\n",
    "echo \"Running action ${SPARK_ACTION}\"\n",
    "case ${SPARK_ACTION} in\n",
    "\"example\")\n",
    "echo \"Running example ARGS $@\"\n",
    "./bin/run-example $@\n",
    ";;\n",
    "\"spark-shell\")\n",
    "./bin/spark-shell --master local[2]\n",
    ";;\n",
    "\"pyspark\")\n",
    "./bin/pyspark --master local[2]\n",
    ";;\n",
    "\"spark-submit-python\")\n",
    " ./bin/spark-submit --packages $2 /opt/tap/$1\n",
    ";;\n",
    "\"spark-submit-apps\")\n",
    " ./bin/spark-submit --packages $3 --class $1 /opt/tap/apps/$2\n",
    ";;\n",
    "\"pytap\")\n",
    "cd /opt/tap/\n",
    "python ${TAP_CODE}\n",
    ";;\n",
    "\"bash\")\n",
    "while true\n",
    "do\n",
    "\techo \"Keep Alive\"\n",
    "\tsleep 10\n",
    "done\n",
    ";;\n",
    "esac\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SparkPI\n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py\n",
    "\n",
    "Use Monte Carlo Method  https://theabbie.github.io/blog/estimate-pi-using-random-numbers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Run sparkExamplePi.sh\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "# Stop\n",
    "docker stop sparkPi\n",
    "\n",
    "# Remove previuos container \n",
    "docker container rm sparkPi\n",
    "\n",
    "docker build ../spark/ --tag tap:spark\n",
    "docker run -e SPARK_ACTION=example --network tap --name sparkPi -it tap:spark SparkPi 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "Running action example\n",
    "Running example ARGS SparkPi 100\n",
    "21/04/18 15:31:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "21/04/18 15:31:59 INFO SparkContext: Running Spark version 3.1.1\n",
    "21/04/18 15:31:59 INFO ResourceUtils: ==============================================================\n",
    "21/04/18 15:31:59 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
    "21/04/18 15:31:59 INFO ResourceUtils: ==============================================================\n",
    "21/04/18 15:31:59 INFO SparkContext: Submitted application: Spark Pi\n",
    "\n",
    "...\n",
    "Pi is roughly 3.1419099141909914\n",
    "21/04/18 15:32:09 INFO SparkUI: Stopped Spark web UI at http://958a11429922:4040\n",
    "21/04/18 15:32:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
    "21/04/18 15:32:09 INFO MemoryStore: MemoryStore cleared\n",
    "21/04/18 15:32:09 INFO BlockManager: BlockManager stopped\n",
    "21/04/18 15:32:09 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
    "21/04/18 15:32:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
    "21/04/18 15:32:09 INFO SparkContext: Successfully stopped SparkContext\n",
    "21/04/18 15:32:09 INFO ShutdownHookManager: Shutdown hook called\n",
    "21/04/18 15:32:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc591bec-ca63-4e7b-86f4-191684261e8f\n",
    "21/04/18 15:32:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-4dc0f0c2-7124-4fc7-9122-b4ea4789f57f\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Spark Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "./sparkShell.sh\n",
    "### docker run -e SPARK_ACTION=spark-shell --network tap -it tap:spark)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "Spark context Web UI available at http://17827060ca34:4040\n",
    "Spark context available as 'sc' (master = local[2], app id = local-1586889603227).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_212)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```scala\n",
    "scala> val textFile=spark.read.textFile(\"/opt/tap/spark/dataset/lotr_characters.csv\");\n",
    "scala> textFile.count();\n",
    "res0: Long = 912\n",
    "\n",
    "scala> textFile.first();\n",
    "\n",
    "res3: String = birth,death,gender,hair,height,name,race,realm,spouse\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resilient Distributed Dataset (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds\n",
    "\n",
    "Spark revolves around the concept of a resilient distributed dataset (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://1.bp.blogspot.com/-wMroEy8Ow-k/WdCUxRefTTI/AAAAAAAABNM/Z14px-DgqGYqPfAfwNIILI9EX-ozLGplQCLcBGAs/s640/apache-spark-streaming-13-638.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "fault-tolerant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://i.imgflip.com/1dzjjc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "collection of elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://mallikarjuna_g.gitbooks.io/spark/content/diagrams/spark-rdds.png)\n",
    "\n",
    "> https://books.japila.pl/apache-spark-internals/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "that can be operated on in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://mallikarjuna_g.gitbooks.io/spark/content/diagrams/spark-rdd-partitioned-distributed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Learning Spark \n",
    "\n",
    "An RDD in Spark is simply an immutable distributed collection of objects. \n",
    "\n",
    "Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \n",
    "\n",
    "RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two ways to create RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "parallelizing an existing collection in your driver program, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "referencing a dataset in an external storage system, \n",
    "such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Py Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Start Py Spark Docker\n",
    "```bash\n",
    "./pyspark.sh \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export JAVA_HOME=/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'findspark' from '/Users/nics/miniforge3/lib/python3.9/site-packages/findspark.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "findspark.find() \n",
    "findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/10 21:25:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.67:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Tap</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Tap>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAppName('Tap').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List\n",
    "data = [1, 2, 3, 4, 5] \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData = sc.parallelize(data)\n",
    "distData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/nics/Dev/GitHub/tap2022/spark/dataset/The Return Of The King_djvu.txt MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An RDD can be also created from external storage\n",
    "# textFile creates a RDD(String) (remember when we use spark.read.file)\n",
    "distFile = sc.textFile(\"/Users/nics/Dev/GitHub/tap2022/spark/dataset/The Return Of The King_djvu.txt\") # Path may be different in your local env\n",
    "distFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "710716"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeOfBook=distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)\n",
    "sizeOfBook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Key Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements by a key.\n",
    "\n",
    "In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = distFile.map(lambda s: (s, 1))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have create a new RDD, let's see what it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"trti ', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('“THE LORD OF THE RINGS” ', 1),\n",
       " ('', 1),\n",
       " ('Pjrt Thttt ', 1),\n",
       " ('', 1),\n",
       " ('THE RETURN ', 1),\n",
       " ('OF THE KING ', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('J.R.R.ToIkien ', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('* BOOK V * ', 1),\n",
       " ('', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can use a reduce function, to count how may times the line appears in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 5807),\n",
       " (\"“THE LORD OF THE RINGS' \", 1),\n",
       " ('V*art One ', 1),\n",
       " ('THE FELLOWSHIP ', 1),\n",
       " ('OF THE RING ', 1),\n",
       " ('J.R.R.ToIkien ', 1),\n",
       " ('Complete Table of Contents ', 1),\n",
       " ('Foreword ', 2),\n",
       " ('Prologue ', 1),\n",
       " ('1 . Concerning Hobbits ', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's order by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered=counts.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 5807),\n",
       " ('\" \"The enemy must have some great need or purpose,\" said Radagast; \"but ',\n",
       "  1),\n",
       " ('\" \"The time of my thought is my own to spend,\" answered Dbin. ', 1),\n",
       " ('\"\\'Give us that, Deal, my love,\" said Smjagol, over his friend\\'s ', 1),\n",
       " ('\"Ass! Fool! Thrice worthy and beloved Barliman! \" said I. \"It\\'s the ', 1),\n",
       " ('\"At the worst,\" said he, \"our Enemy knows that we have it not and ', 1),\n",
       " ('\"I am afraid we must go back to the Road here for a while,\\' said ', 1),\n",
       " ('\"I will do that,\" he said, and rode off as if the Nine were after ', 1),\n",
       " ('\"So you have come, Gandalf,\" he said to me gravely; but in his eyes ', 1),\n",
       " ('\"Talking,\\' said Bilbo. \\'There was a deal of talk, and everyone had an ',\n",
       "  1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered.takeOrdered(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's do a better analysis\n",
    "Which is the most frequent word in the book ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "words=distFile.flatMap(lambda line:line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"trti',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '“THE',\n",
       " 'LORD',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'RINGS”',\n",
       " '',\n",
       " '',\n",
       " 'Pjrt',\n",
       " 'Thttt',\n",
       " '',\n",
       " '',\n",
       " 'THE',\n",
       " 'RETURN',\n",
       " '',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'KING',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'J.R.R.ToIkien',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '*',\n",
       " 'BOOK',\n",
       " 'V',\n",
       " '*',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Chapter',\n",
       " '1',\n",
       " '.',\n",
       " 'Minas',\n",
       " 'Tirith',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Pippin',\n",
       " 'looked',\n",
       " 'out',\n",
       " 'from',\n",
       " 'the',\n",
       " 'shelter',\n",
       " 'of',\n",
       " 'Gandalf',\n",
       " 's',\n",
       " 'cloak.',\n",
       " 'He',\n",
       " 'wondered',\n",
       " 'if',\n",
       " '',\n",
       " 'he',\n",
       " 'was',\n",
       " 'awake',\n",
       " 'or',\n",
       " 'still',\n",
       " 'sleeping,',\n",
       " 'still',\n",
       " 'in',\n",
       " 'the',\n",
       " 'swift',\n",
       " '-moving',\n",
       " 'dream',\n",
       " 'in',\n",
       " 'which',\n",
       " 'he',\n",
       " '',\n",
       " 'had',\n",
       " 'been',\n",
       " 'wrapped',\n",
       " 'so',\n",
       " 'long',\n",
       " 'since',\n",
       " 'the',\n",
       " 'great',\n",
       " 'ride',\n",
       " 'began.',\n",
       " 'The',\n",
       " 'dark',\n",
       " 'world',\n",
       " 'was',\n",
       " '',\n",
       " 'rushing',\n",
       " 'by',\n",
       " 'and',\n",
       " 'the',\n",
       " 'wind',\n",
       " 'sang']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Great, let's assign a counter and then sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wordCounters=words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 21555),\n",
       " ('“THE', 1),\n",
       " ('LORD', 1),\n",
       " ('OF', 3),\n",
       " ('THE', 9),\n",
       " (\"RINGS'\", 1),\n",
       " ('V*art', 1),\n",
       " ('One', 67),\n",
       " ('FELLOWSHIP', 1),\n",
       " ('RING', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounters.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ok I want to sort now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 14916),\n",
       " ('the', 8035),\n",
       " ('and', 5811),\n",
       " ('of', 3964),\n",
       " ('to', 2768),\n",
       " ('a', 2337),\n",
       " ('in', 2004),\n",
       " ('he', 1923),\n",
       " ('that', 1644),\n",
       " ('was', 1505),\n",
       " ('his', 1301),\n",
       " ('I', 1269),\n",
       " ('it', 1081),\n",
       " ('they', 1041),\n",
       " ('you', 990),\n",
       " ('for', 903),\n",
       " ('as', 898),\n",
       " ('not', 890),\n",
       " ('with', 868),\n",
       " ('said', 847),\n",
       " ('had', 810),\n",
       " ('is', 805),\n",
       " ('at', 759),\n",
       " ('all', 696),\n",
       " ('on', 688),\n",
       " ('have', 648),\n",
       " ('be', 646),\n",
       " ('but', 635),\n",
       " ('were', 617),\n",
       " ('from', 594),\n",
       " ('And', 552),\n",
       " ('But', 531),\n",
       " ('will', 516),\n",
       " ('their', 484),\n",
       " ('there', 482),\n",
       " ('The', 469),\n",
       " ('now', 451),\n",
       " ('no', 423),\n",
       " ('came', 422),\n",
       " ('if', 408),\n",
       " ('or', 404),\n",
       " ('great', 401),\n",
       " ('we', 396),\n",
       " ('He', 394),\n",
       " ('my', 367),\n",
       " ('are', 362),\n",
       " ('by', 357),\n",
       " ('him', 353),\n",
       " ('out', 351),\n",
       " ('up', 347),\n",
       " ('would', 319),\n",
       " ('your', 317),\n",
       " ('them', 307),\n",
       " ('could', 305),\n",
       " ('this', 298),\n",
       " ('into', 291),\n",
       " ('like', 287),\n",
       " ('upon', 275),\n",
       " ('then', 266),\n",
       " ('when', 260),\n",
       " ('one', 258),\n",
       " ('so', 257),\n",
       " ('been', 256),\n",
       " ('long', 256),\n",
       " ('more', 254),\n",
       " ('some', 253),\n",
       " ('come', 249),\n",
       " ('Sam', 248),\n",
       " ('went', 247),\n",
       " ('than', 243),\n",
       " ('before', 240),\n",
       " ('down', 240),\n",
       " ('what', 237),\n",
       " (\"'I\", 236),\n",
       " ('Then', 233),\n",
       " ('do', 232),\n",
       " ('shall', 222),\n",
       " ('last', 221),\n",
       " ('still', 214),\n",
       " ('It', 207),\n",
       " ('its', 205),\n",
       " ('about', 205),\n",
       " ('men', 197),\n",
       " ('many', 196),\n",
       " ('go', 195),\n",
       " ('has', 194),\n",
       " ('any', 192),\n",
       " ('back', 192),\n",
       " ('Frodo', 190),\n",
       " ('me', 190),\n",
       " ('looked', 188),\n",
       " ('must', 185),\n",
       " ('only', 181),\n",
       " ('far', 178),\n",
       " ('They', 177),\n",
       " ('see', 172),\n",
       " ('yet', 172),\n",
       " ('did', 171),\n",
       " ('said.', 170),\n",
       " ('For', 169),\n",
       " ('saw', 167),\n",
       " ('stood', 166),\n",
       " ('who', 164),\n",
       " ('over', 163),\n",
       " ('seemed', 160),\n",
       " ('she', 159),\n",
       " ('an', 156),\n",
       " ('even', 156),\n",
       " ('Gandalf', 155),\n",
       " ('where', 155),\n",
       " ('Pippin', 154),\n",
       " ('may', 153),\n",
       " ('our', 150),\n",
       " ('away', 148),\n",
       " ('through', 140),\n",
       " ('passed', 139),\n",
       " ('should', 139),\n",
       " ('old', 137),\n",
       " ('can', 137),\n",
       " ('Aragorn', 136),\n",
       " ('her', 135),\n",
       " ('little', 133),\n",
       " ('dark', 130),\n",
       " ('There', 129),\n",
       " ('other', 129),\n",
       " (\"'But\", 126),\n",
       " ('Merry', 125),\n",
       " ('him,', 123),\n",
       " ('set', 122),\n",
       " ('while', 122),\n",
       " ('turned', 121),\n",
       " ('it,', 121),\n",
       " ('time', 120),\n",
       " ('am', 119),\n",
       " ('him.', 118),\n",
       " ('though', 118),\n",
       " ('Lord', 116),\n",
       " ('heard', 116),\n",
       " ('which', 115),\n",
       " ('very', 115),\n",
       " ('under', 114),\n",
       " ('day', 112),\n",
       " ('At', 112),\n",
       " ('rode', 112),\n",
       " ('made', 111),\n",
       " ('Mr.', 111),\n",
       " ('know', 109),\n",
       " ('way', 109),\n",
       " ('black', 108),\n",
       " ('hope', 106),\n",
       " ('road', 106),\n",
       " ('those', 105),\n",
       " ('again', 104),\n",
       " ('such', 104),\n",
       " ('much', 104),\n",
       " ('light', 103),\n",
       " ('behind', 103),\n",
       " ('ever', 103),\n",
       " ('after', 101),\n",
       " ('left', 101),\n",
       " ('until', 101),\n",
       " ('it.', 100),\n",
       " ('You', 100),\n",
       " ('eyes', 100),\n",
       " ('A', 99),\n",
       " ('days', 99),\n",
       " ('them.', 98),\n",
       " ('things', 98),\n",
       " ('going', 97),\n",
       " ('us', 96),\n",
       " ('thought', 95),\n",
       " (\"'And\", 94),\n",
       " ('lay', 94),\n",
       " ('own', 93),\n",
       " ('first', 92),\n",
       " ('beyond', 92),\n",
       " ('soon', 92),\n",
       " ('too', 91),\n",
       " ('hand', 90),\n",
       " ('think', 90),\n",
       " ('fell', 89),\n",
       " ('might', 88),\n",
       " ('towards', 88),\n",
       " ('here', 87),\n",
       " ('you,', 87),\n",
       " ('In', 87),\n",
       " ('high', 86),\n",
       " ('took', 86),\n",
       " ('Gondor', 84),\n",
       " ('these', 84),\n",
       " ('City', 83),\n",
       " ('grey', 83),\n",
       " ('take', 83),\n",
       " ('them,', 83),\n",
       " ('above', 82),\n",
       " ('Faramir', 81),\n",
       " ('off', 81),\n",
       " ('two', 80),\n",
       " ('white', 79),\n",
       " ('fear', 79)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsSorted=wordCounters.takeOrdered(200, key = lambda x: -x[1])\n",
    "wordsSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblio\n",
    "- https://www.kdnuggets.com/2017/08/three-apache-spark-apis-rdds-dataframes-datasets.html\n",
    "- https://www.slideshare.net/differentsachin/apache-spark-introduction-and-resilient-distributed-dataset-basics-and-deep-dive\n",
    "- https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/\n",
    "- https://www.slideshare.net/taposhdr/resilient-distributed-datasets\n",
    "- http://vishnuviswanath.com/spark_rdd.html\n",
    "- https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/\n",
    "- https://www.educba.com/rdd-in-spark/\n",
    "- https://www.javahelps.com/2019/02/spark-03-understanding-resilient.html\n",
    "- https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm\n",
    "- https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf\n",
    "- https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2022 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
