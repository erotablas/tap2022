{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Spark Kafka Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://static.wixstatic.com/media/f17a52_84852646da5a4e37837a12cb610b2ad8~mv2.png/v1/fill/w_1000,h_673,al_c,usm_0.66_1.00_0.01/f17a52_84852646da5a4e37837a12cb610b2ad8~mv2.png)\n",
    "[Source](https://www.dataneb.com/post/analyzing-twitter-texts-spark-streaming-example-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"jumbotron\">\n",
    "    <center>\n",
    "        <b>Sentiment Analysis</b> of streaming twitter data using Flume/Kafka/Spark\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://i.imgflip.com/40j9cu.jpg)\n",
    "[NicsMeme](https://imgflip.com/i/40j9cu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Workflow Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1) Model Building\n",
    "\n",
    "Goal: Build Spark Mlib pipeline to classify whether the tweet contains hate speech or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Focus is not to build a very accurate classification model but to see how to use any model and return results on streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2) Predict and Return Results\n",
    "\n",
    "Once we get a new the tweet (and we will do using kafka streaming), \n",
    "we pass the data into the machine learning pipeline we created and return the predicted sentiment from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/cuofano.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# init 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/22 08:41:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.67:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TapDataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x116cba850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find( ) \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TapDataFrame\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](http://thejoyofgeek.net/wp-content/uploads/2016/08/robotmask.jpg)\n",
    "[S2E4](http://thejoyofgeek.net/mr-robot-init_1-review-s2e4/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Let's Start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Trainset \n",
    "***SentiTUT*** \n",
    "\n",
    "http://www.di.unito.it/~tutreeb/sentipolc-evalita16/data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# idtwitter\tsubj\topos\toneg\tiro\tlpos\tlneg\ttop\ttext\n",
    "\n",
    "schema = tp.StructType([\n",
    "    tp.StructField(name= 'id', dataType= tp.StringType(),  nullable= True),\n",
    "    tp.StructField(name= 'subjective',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'positive',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'negative',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'ironic',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'lpositive',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'lnegative',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'top',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'text',       dataType= tp.StringType(),   nullable= True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, subjective: int, positive: int, negative: int, ironic: int, lpositive: int, lnegative: int, top: int, text: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dataset  \n",
    "training_set = spark.read.csv('../spark/dataset/training_set_sentipolc16.csv',\n",
    "                         schema=schema,\n",
    "                         header=True,\n",
    "                         sep=',')\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|positive|count|\n",
      "+--------+-----+\n",
      "|       1| 2051|\n",
      "|       0| 5359|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training_set.show(truncate=False)\n",
    "training_set.groupBy(\"positive\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'text' , outputCol= 'tokens', pattern= '\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 3: create a word vector of the size 100\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 4: Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1600/1*DyD3VP18IV3-lXcKMbyr5w.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelSummary=pipelineFit.stages[-1].summary\n",
    "modelSummary \n",
    "# https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/classification/LogisticRegressionSummary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelSummary.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/accuracy.jpg)\n",
    "[DeepLearningNewsAndMemes](https://www.facebook.com/DeepLearningNewsAndMemes/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tweetDf = spark.createDataFrame([\"False illusioni, sgradevoli realtà Mario Monti http://t.co/WOmMCITs via @AddToAny\"], tp.StringType()).toDF(\"text\")\n",
    "tweetDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipelineFit.transform(tweetDf).select('text','tokens','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tweetDf = spark.createDataFrame([\"Mario Monti sul Corriere: la fotografia più illuminante sulla delicata situazione attuale http://t.co/YbuNZMOJ\"], tp.StringType()).toDF(\"text\")\n",
    "tweetDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipelineFit.transform(tweetDf).select('tokens','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipelineFit.save(\"../spark/dataset/model.save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = modelSummary.fMeasureByThreshold\n",
    "fMeasure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)')\n",
    "maxFMeasure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bestThreshold=fMeasure.where(fMeasure['F-Measure'] == 0.49136276391554706)\n",
    "bestThreshold.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.setThreshold(0.2547534323886861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelSummary=pipelineFit.stages[-1].summary\n",
    "modelSummary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelSummary=pipelineFit.stages[-1].summary\n",
    "modelSummary.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://i.imgflip.com/40mt0s.jpg)\n",
    "[NicsMeme](https://imgflip.com/i/40mt0s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Another Approach: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 3: create a word vector of the size 100\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"vector\", numFeatures=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 4: Logistic Regression Model\n",
    "modelNaive =  NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol= 'vector', labelCol= 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipelineNaive = Pipeline(stages= [stage_1, stage_2, hashingTF, modelNaive])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipelineNaiveFit = pipelineNaive.fit(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipelineNaiveFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# select example rows to display.\n",
    "predictions = pipelineNaiveFit.transform(training_set)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"positive\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Yet another one \n",
    "https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "model = LogisticRegression(featuresCol= 'features', labelCol= 'positive',maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1073.9 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:15 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "22/05/22 09:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n"
     ]
    }
   ],
   "source": [
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 09:14:06 WARN DAGScheduler: Broadcasting large task binary with size 1116.0 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9257759784075573"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSummary=pipelineFit.stages[-1].summary\n",
    "modelSummary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = tp.StructType([tp.StructField(\"text\",tp.StringType(),True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                          |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|Mario Monti sul Corriere: la fotografia più illuminante sulla delicata situazione attuale http://t.co/YbuNZMOJ|\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       1.0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 09:21:05 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n",
      "22/05/22 09:21:05 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n",
      "22/05/22 09:21:05 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n"
     ]
    }
   ],
   "source": [
    "tweetDf = spark.createDataFrame([\"Mario Monti sul Corriere: la fotografia più illuminante sulla delicata situazione attuale http://t.co/YbuNZMOJ\"], tp.StringType()).toDF(\"text\")\n",
    "tweetDf.select(\"text\").show(truncate=False)\n",
    "tw2=pipelineFit.transform(tweetDf)\n",
    "tw2.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Le 5 sgradevoli realtà di cui Berlusconi dovrebbe rendersi personalmente conto http://t.co/G3u1iF9n Mario Monti non usa mezzi termini|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 09:21:47 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n",
      "22/05/22 09:21:47 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n",
      "22/05/22 09:21:47 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n"
     ]
    }
   ],
   "source": [
    "tweetDf = spark.createDataFrame([\"Le 5 sgradevoli realtà di cui Berlusconi dovrebbe rendersi personalmente conto http://t.co/G3u1iF9n Mario Monti non usa mezzi termini\"], tp.StringType()).toDF(\"text\")\n",
    "tweetDf.select(\"text\").show(truncate=False)\n",
    "tw2=pipelineFit.transform(tweetDf)\n",
    "tw2.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|text          |\n",
      "+--------------+\n",
      "|Monti mi piace|\n",
      "+--------------+\n",
      "\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       1.0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 09:29:06 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n",
      "22/05/22 09:29:06 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n",
      "22/05/22 09:29:06 WARN DAGScheduler: Broadcasting large task binary with size 1106.7 KiB\n"
     ]
    }
   ],
   "source": [
    "tweetDf = spark.createDataFrame([\"Monti mi piace\"], tp.StringType()).toDF(\"text\")\n",
    "tweetDf.select(\"text\").show(truncate=False)\n",
    "tw2=pipelineFit.transform(tweetDf)\n",
    "tw2.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 09:25:04 WARN DAGScheduler: Broadcasting large task binary with size 1116.1 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.971300461118872"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipelineFit.transform(training_set)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"positive\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 09:26:14 WARN DAGScheduler: Broadcasting large task binary with size 1110.9 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9257759784075573"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.positive == predictions.prediction).count() / float(training_set.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Biblio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/\n",
    "* https://www.kdnuggets.com/2018/02/machine-learning-algorithm-2118.html\n",
    "* https://towardsdatascience.com/sentiment-analysis-using-logistic-regression-and-naive-bayes-16b806eb4c4b\n",
    "* http://www.di.unito.it/~tutreeb/sentipolc-evalita16/data.html\n",
    "* http://www.di.unito.it/~tutreeb/ironita-evalita18/data.html\n",
    "* https://towardsdatascience.com/sentiment-analysis-and-emotion-recognition-in-italian-using-bert-92f5c8fe8a2\n",
    "* https://github.com/charlesmalafosse/open-dataset-for-sentiment-analysis\n",
    "* https://iris.unito.it/retrieve/handle/2318/146318/175020/21_Paper.pdf\n",
    "* https://aperto.unito.it/retrieve/handle/2318/1698302/496918/Sentiment%20analysis%20on%20Italian%20tweets.pdf\n",
    "* https://iopscience.iop.org/article/10.1088/1742-6596/1000/1/012130/pdf\n",
    "* https://towardsdatascience.com/sentiment-analysis-using-logistic-regression-and-naive-bayes-16b806eb4c4b\n",
    "* https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "* https://dzone.com/articles/streaming-machine-learning-pipeline-for-sentiment\n",
    "* https://databricks.com/wp-content/uploads/2015/10/STEP-3-Sentiment_Analysis.html\n",
    "* https://github.com/P7h/Spark-MLlib-Twitter-Sentiment-Analysis/blob/master/src/main/scala/org/p7h/spark/sentiment/mllib/MLlibSentimentAnalyzer.scala\n",
    "* https://www.researchgate.net/publication/315913579_An_Apache_Spark_Implementation_for_Sentiment_Analysis_on_Twitter_Data\n",
    "* https://medium.com/analytics-vidhya/congressional-tweets-using-sentiment-analysis-to-cluster-members-of-congress-in-pyspark-10afa4d1556e\n",
    "* https://developer.hpe.com/blog/streaming-ml-pipeline-for-sentiment-analysis-using-apache-apis-kafka-spark-and-drill-part-2/\n",
    "* https://dataespresso.com/en/2017/10/24/comparison-between-naive-bayes-and-logistic-regression/#:~:text=Na%C3%AFve%20Bayes%20has%20a%20naive,belonging%20to%20a%20certain%20class.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2022 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
