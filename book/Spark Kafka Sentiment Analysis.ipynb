{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Spark Kafka Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://static.wixstatic.com/media/f17a52_84852646da5a4e37837a12cb610b2ad8~mv2.png/v1/fill/w_1000,h_673,al_c,usm_0.66_1.00_0.01/f17a52_84852646da5a4e37837a12cb610b2ad8~mv2.png)\n",
    "[Source](https://www.dataneb.com/post/analyzing-twitter-texts-spark-streaming-example-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"jumbotron\">\n",
    "    <center>\n",
    "        <b>Sentiment Analysis</b> of streaming twitter data using Flume/Kafka/Spark\n",
    "    </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://i.imgflip.com/40j9cu.jpg)\n",
    "[NicsMeme](https://imgflip.com/i/40j9cu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Workflow Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1) Model Building\n",
    "\n",
    "Goal: Build Spark Mlib pipeline to classify whether the tweet contains hate speech or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Focus is not to build a very accurate classification model but to see how to use any model and return results on streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2) Initialize Spark Streaming \n",
    "\n",
    "Once the model is built, we need to define the source where to get tweet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3) Stream Data\n",
    "\n",
    "Start stream -> the Spark Streaming API will receive the data after a specified duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4) Predict and Return Results\n",
    "\n",
    "Once we receive the tweet text, we pass the data into the machine learning pipeline we created and return the predicted sentiment from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/cuofano.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# init 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.22.84.91:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TapDataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa7909210d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find( ) \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"TapDataFrame\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](http://thejoyofgeek.net/wp-content/uploads/2016/08/robotmask.jpg)\n",
    "[S2E4](http://thejoyofgeek.net/mr-robot-init_1-review-s2e4/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Let's Start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Trainset \n",
    "***SentiTUT*** \n",
    "\n",
    "http://www.di.unito.it/~tutreeb/sentipolc-evalita16/data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# idtwitter\tsubj\topos\toneg\tiro\tlpos\tlneg\ttop\ttext\n",
    "\n",
    "schema = tp.StructType([\n",
    "    tp.StructField(name= 'id', dataType= tp.StringType(),  nullable= True),\n",
    "    tp.StructField(name= 'subjective',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'positive',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'negative',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'ironic',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'lpositive',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'lnegative',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'top',       dataType= tp.IntegerType(),  nullable= True),\n",
    "    tp.StructField(name= 'tweet',       dataType= tp.StringType(),   nullable= True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|positive|count|\n",
      "+--------+-----+\n",
      "|       1| 2051|\n",
      "|       0| 5359|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the dataset  \n",
    "training_set = spark.read.csv('../spark/dataset/training_set_sentipolc16.csv',\n",
    "                         schema=schema,\n",
    "                         header=True,\n",
    "                         sep=',')\n",
    "\n",
    "#training_set.show(truncate=False)\n",
    "training_set.groupBy(\"positive\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://www.meme-arsenal.com/memes/a05a53a96e890dee5a52d1156c01eb06.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 1: tokenize the tweet text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 3: create a word vector of the size 100\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 4: Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1600/1*DyD3VP18IV3-lXcKMbyr5w.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_d896a7e3621c"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.classification.BinaryLogisticRegressionTrainingSummary at 0x7fa790930b10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSummary=pipelineFit.stages[-1].summary\n",
    "modelSummary \n",
    "# https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/classification/LogisticRegressionSummary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7379217273954116"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSummary.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/accuracy.jpg)\n",
    "[DeepLearningNewsAndMemes](https://www.facebook.com/DeepLearningNewsAndMemes/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+\n",
      "|tweet                                                                            |\n",
      "+---------------------------------------------------------------------------------+\n",
      "|False illusioni, sgradevoli realtà Mario Monti http://t.co/WOmMCITs via @AddToAny|\n",
      "+---------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf = spark.createDataFrame([\"False illusioni, sgradevoli realtà Mario Monti http://t.co/WOmMCITs via @AddToAny\"], tp.StringType()).toDF(\"tweet\")\n",
    "tweetDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+----------+\n",
      "|tweet                                                                            |tokens                                                                                   |prediction|\n",
      "+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+----------+\n",
      "|False illusioni, sgradevoli realtà Mario Monti http://t.co/WOmMCITs via @AddToAny|[false, illusioni, sgradevoli, realt, mario, monti, http, t, co, wommcits, via, addtoany]|0.0       |\n",
      "+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineFit.transform(tweetDf).select('tweet','tokens','prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|tweet                          |\n",
      "+-------------------------------+\n",
      "|Tutti amano le ruspe di salvini|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf = spark.createDataFrame([\"Tutti amano le ruspe di salvini\"], tp.StringType()).toDF(\"tweet\")\n",
    "tweetDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|Tutti amano le ru...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineFit.transform(tweetDf).select('tweet','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipelineFit.save(\"../spark/dataset/model.save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|         threshold|           F-Measure|\n",
      "+------------------+--------------------+\n",
      "|0.9041434497991898| 0.01926782273603083|\n",
      "|0.8723788810130683|0.025911708253358926|\n",
      "|0.8477174540710809|0.032489249880554225|\n",
      "|0.8233040251643247| 0.04091341579448145|\n",
      "|0.8081594480454658| 0.05198487712665407|\n",
      "|0.7926974581873518| 0.05652378709373528|\n",
      "|0.7776043563538639| 0.06009389671361502|\n",
      "|0.7613078168002123| 0.06644829199812821|\n",
      "|0.7568858364149114| 0.07182835820895524|\n",
      "|0.7452266995079002| 0.07717340771734078|\n",
      "|0.7372997095753309| 0.08244557665585919|\n",
      "| 0.724341403421372| 0.08494921514312095|\n",
      "|0.7100560309887609| 0.09019788311090657|\n",
      "|0.7019862099427137| 0.09449541284403669|\n",
      "|0.6888544933400849| 0.09967992684042067|\n",
      "|0.6707325625546149| 0.10209662716499544|\n",
      "|0.6610687822086312| 0.10540663334847797|\n",
      "|0.6533578485988583| 0.11141304347826088|\n",
      "|0.6395454679995733|  0.1146726862302483|\n",
      "| 0.628057391317428| 0.11791179117911792|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = modelSummary.fMeasureByThreshold\n",
    "fMeasure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    max(F-Measure)|\n",
      "+------------------+\n",
      "|0.4875286916602907|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)')\n",
    "maxFMeasure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|         threshold|         F-Measure|\n",
      "+------------------+------------------+\n",
      "|0.2251829398572779|0.4875286916602907|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestThreshold=fMeasure.where(fMeasure['F-Measure'] == 0.4875286916602907)\n",
    "bestThreshold.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_3f94ad6121c6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.setThreshold(0.4875286916602907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7379217273954116"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSummary=pipelineFit.stages[-1].summary\n",
    "modelSummary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7369770580296896"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSummary=pipelineFit.stages[-1].summary\n",
    "modelSummary.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://i.imgflip.com/40mt0s.jpg)\n",
    "[NicsMeme](https://imgflip.com/i/40mt0s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Another Approach: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 3: create a word vector of the size 100\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"vector\", numFeatures=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define stage 4: Logistic Regression Model\n",
    "modelNaive =  NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol= 'vector', labelCol= 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipelineNaive = Pipeline(stages= [stage_1, stage_2, hashingTF, modelNaive])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipelineNaiveFit = pipelineNaive.fit(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_7ac5943ef172"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelineNaiveFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+--------+--------+------+---------+---------+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                id|subjective|positive|negative|ironic|lpositive|lnegative|top|               tweet|              tokens|      filtered_words|              vector|       rawPrediction|         probability|prediction|\n",
      "+------------------+----------+--------+--------+------+---------+---------+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|122449983151669248|         1|       0|       1|     0|        0|        1|  1|\"Intanto la parti...|[intanto, la, par...|[intanto, la, par...|(20,[1,2,5,7,8,9,...|[-60.853495557968...|[0.71615635721972...|       0.0|\n",
      "|125485104863780865|         1|       0|       1|     0|        0|        1|  1|False illusioni, ...|[false, illusioni...|[false, illusioni...|(20,[1,2,5,8,9,12...|[-32.016758584080...|[0.71256437318795...|       0.0|\n",
      "|125513454315507712|         1|       0|       1|     0|        0|        1|  1|False illusioni, ...|[false, illusioni...|[false, illusioni...|(20,[1,2,3,4,5,7,...|[-50.605038834543...|[0.76122396467856...|       0.0|\n",
      "|125524238290522113|         1|       0|       1|     0|        0|        1|  1|Mario Monti: Berl...|[mario, monti, be...|[mario, monti, be...|(20,[1,2,3,4,5,8,...|[-38.528212542914...|[0.78979682665815...|       0.0|\n",
      "|125527933224886272|         1|       0|       1|     0|        0|        1|  1|Mario Monti: Berl...|[mario, monti, be...|[mario, monti, be...|(20,[1,2,3,4,5,8,...|[-57.985170261255...|[0.88465000502438...|       0.0|\n",
      "|125530285164072961|         1|       1|       1|     0|        1|        1|  1|False illusioni, ...|[false, illusioni...|[false, illusioni...|(20,[0,1,2,5,6,8,...|[-60.387657883083...|[0.62037949054680...|       0.0|\n",
      "|125533343482789889|         1|       0|       1|     0|        0|        1|  1|L'attacco di Mari...|[l, attacco, di, ...|[l, attacco, di, ...|(20,[1,2,3,5,7,8,...|[-51.363361434018...|[0.85219092176332...|       0.0|\n",
      "|125633929217708032|         1|       1|       0|     0|        1|        0|  1|Mario Monti sul C...|[mario, monti, su...|[mario, monti, su...|(20,[2,4,5,6,7,8,...|[-45.213274592424...|[0.75926105549426...|       0.0|\n",
      "|125642756147265536|         1|       0|       1|     0|        0|        1|  1|Le 5 sgradevoli r...|[le, 5, sgradevol...|[le, 5, sgradevol...|(20,[1,2,3,4,5,6,...|[-61.081004660662...|[0.62377550613351...|       0.0|\n",
      "|125692702145785856|         1|       0|       1|     0|        0|        1|  1|False illusioni, ...|[false, illusioni...|[false, illusioni...|(20,[1,2,3,4,5,7,...|[-55.295901304654...|[0.75916005764850...|       0.0|\n",
      "|125695266887184384|         1|       0|       1|     1|        0|        1|  1|Mario Monti: c'è ...|[mario, monti, c,...|[mario, monti, c,...|(20,[1,2,5,8,9,10...|[-55.372692489681...|[0.78556992305300...|       0.0|\n",
      "|125838624670490624|         1|       0|       1|     1|        0|        1|  1|Ma a quanta gente...|[ma, a, quanta, g...|[ma, quanta, gent...|(20,[1,2,4,8,9,10...|[-60.401217417684...|[0.59411212502712...|       0.0|\n",
      "|125949521627840512|         1|       0|       1|     0|        0|        1|  1|Mario Monti: Fals...|[mario, monti, fa...|[mario, monti, fa...|(20,[0,2,5,8,9,12...|[-51.230167940941...|[0.73660999266030...|       0.0|\n",
      "|126527896218107904|         1|       0|       1|     0|        0|        1|  1|@mauryred82 l'ho ...|[mauryred82, l, h...|[mauryred82, l, h...|(20,[1,2,3,4,5,6,...|[-65.920951738714...|[0.82044496094775...|       0.0|\n",
      "|127100968415395841|         1|       0|       1|     0|        0|        1|  1|Ascolti Mario Mon...|[ascolti, mario, ...|[ascolti, mario, ...|(20,[1,2,3,4,5,8,...|[-58.251628300568...|[0.76336715477402...|       0.0|\n",
      "|127137847491821568|         1|       0|       1|     1|        1|        0|  1|#la7 ma perche' M...|[la7, ma, perche,...|[la7, ma, perche,...|(20,[2,7,8,9,10,1...|[-45.058645264914...|[0.62615088476820...|       0.0|\n",
      "|128787344999460865|         1|       0|       1|     0|        0|        1|  1|Perché non ha sen...|[perch, non, ha, ...|[perch, non, ha, ...|(20,[1,3,4,5,8,9,...|[-38.532165959167...|[0.80671766073653...|       0.0|\n",
      "|129143970163990528|         1|       0|       1|     1|        0|        0|  1|Mario Monti è con...|[mario, monti, co...|[mario, monti, co...|(20,[0,1,2,4,5,6,...|[-55.200690663974...|[0.78352099255760...|       0.0|\n",
      "|130172208772419585|         1|       1|       0|     0|        1|        0|  1|@riotta sono piu'...|[riotta, sono, pi...|[riotta, sono, pi...|(20,[1,2,3,4,7,8,...|[-35.698969071701...|[0.74910366055591...|       0.0|\n",
      "|130592030031228929|         1|       0|       1|     0|        0|        1|  1|Mario #Monti: La ...|[mario, monti, la...|[mario, monti, la...|(20,[0,2,3,5,6,8,...|[-71.332891284446...|[0.61801561896488...|       0.0|\n",
      "+------------------+----------+--------+--------+------+---------+---------+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select example rows to display.\n",
    "predictions = pipelineNaiveFit.transform(training_set)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.722132253711201\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"positive\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Biblio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/\n",
    "* https://www.kdnuggets.com/2018/02/machine-learning-algorithm-2118.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "rise": {
   "enable_chalkboard": "true",
   "scroll": "true",
   "theme": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
